{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import time\n",
    "import torch\n",
    "import sklearn.datasets\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.datasets import FashionMNIST as FMNIST\n",
    "from torchvision.datasets import EMNIST\n",
    "import torchvision.transforms as transforms\n",
    "import tensorflow as tf\n",
    "import random as r\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam, SGD, Adagrad, Adadelta, RMSprop\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Activation, BatchNormalization\n",
    "import sklearn\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import joblib\n",
    "from tqdm import tqdm_notebook\n",
    "import copy\n",
    "\n",
    "import Config\n",
    "import Dataloader as DL\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "42\n",
      "(494021,)\n",
      "(494021, 41)\n",
      "{'normal': 0, 'u2r': 1, 'dos': 2, 'r2l': 3, 'probe': 4}\n"
     ]
    }
   ],
   "source": [
    "path=\"../../Data/\"\n",
    "attacks_types = {\n",
    "    'normal': 'normal','back': 'dos','buffer_overflow': 'u2r','ftp_write': 'r2l','guess_passwd': 'r2l',\n",
    "'imap': 'r2l','ipsweep': 'probe','land': 'dos','loadmodule': 'u2r','multihop': 'r2l','neptune': 'dos',\n",
    "'nmap': 'probe','perl': 'u2r','phf': 'r2l','pod': 'dos','portsweep': 'probe','rootkit': 'u2r','satan': 'probe',\n",
    "'smurf': 'dos','spy': 'r2l','teardrop': 'dos','warezclient': 'r2l','warezmaster': 'r2l',\n",
    "}\n",
    "cols =\"\"\"duration,protocol_type,service,flag,src_bytes,dst_bytes,land,wrong_fragment,\n",
    "urgent,hot,num_failed_logins,logged_in,num_compromised,root_shell,su_attempted,num_root,\n",
    "num_file_creations,num_shells,num_access_files,num_outbound_cmds,is_host_login,is_guest_login,\n",
    "count,srv_count,serror_rate,srv_serror_rate,rerror_rate,srv_rerror_rate,same_srv_rate,\n",
    "diff_srv_rate,srv_diff_host_rate,dst_host_count,dst_host_srv_count,dst_host_same_srv_rate,\n",
    "dst_host_diff_srv_rate,dst_host_same_src_port_rate,dst_host_srv_diff_host_rate,\n",
    "dst_host_serror_rate,dst_host_srv_serror_rate,dst_host_rerror_rate,dst_host_srv_rerror_rate\"\"\"\n",
    "  \n",
    "columns =[]\n",
    "for c in cols.split(','):\n",
    "    if(c.strip()):\n",
    "       columns.append(c.strip())\n",
    "print(len(columns))\n",
    "columns.append('target')\n",
    "print(len(columns))\n",
    "\n",
    "attack_categories=[\"dos\",\"u2r\",\"r2l\",'probe','normal']\n",
    "df = pd.read_csv(path+\"kddcup.data_10_percent.gz\", names = columns)\n",
    "df['Attack Type'] = df.target.apply(lambda r:attacks_types[r[:-1]])\n",
    "del df['target']\n",
    "df.head()\n",
    "num_cols = df._get_numeric_data().columns\n",
    "  \n",
    "cate_cols = list(set(df.columns)-set(num_cols))\n",
    "cate_cols.remove('Attack Type')\n",
    "def getuniquevalues(columnname):\n",
    "    values={}\n",
    "    i=0\n",
    "    for entry in df[columnname]:\n",
    "        if entry not in values:\n",
    "            values[entry]=i\n",
    "            i+=1\n",
    "    return values\n",
    "for col in cate_cols:\n",
    "    df[col]=df[col].map(getuniquevalues(col))\n",
    "data=df.to_numpy()\n",
    "Y=df['Attack Type'].map(getuniquevalues('Attack Type'))\n",
    "Y=Y.to_numpy()\n",
    "X=data[:,:-1]\n",
    "print(Y.shape)\n",
    "print(X.shape)\n",
    "print(getuniquevalues('Attack Type'))\n",
    "def normalized(x,y):\n",
    "    x, x_test, y, y_test = sklearn.model_selection.train_test_split(x, y, shuffle=True)\n",
    "    scaler = sklearn.preprocessing.Normalizer().fit(x)\n",
    "    x = scaler.transform(x)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    # changes data to pytorch's tensors\n",
    "    x = torch.from_numpy(x).float()\n",
    "    y = torch.from_numpy(y).long()\n",
    "    x_test = torch.from_numpy(x_test).float()\n",
    "    y_test = torch.from_numpy(y_test).long()\n",
    "    return x.numpy(), x_test.numpy(), y.numpy(), y_test.numpy(), scaler\n",
    "xtrain, x_test, ytrain, y_test,scaler= normalized(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate one random vector of desired length and generation type\n",
    "def generate_vector(vector_length, vector_type, param):\n",
    "    #Check for Gaussian kernel\n",
    "    if vector_type == \"Gaussian\":\n",
    "        #parse for mu\n",
    "        mu = param[\"mu\"]\n",
    "        #parse for sigma\n",
    "        sigma = param[\"sigma\"]\n",
    "        #return vector generated from kernel\n",
    "        return np.random.normal(mu, sigma, vector_length)\n",
    "    else:\n",
    "        #all other kernels are currently unsupported\n",
    "        raise Exception(\"Vector type %s not recognized. Abort.\\n\" % vector_type)\n",
    "\n",
    "def vanilla(param):\n",
    "\n",
    "    #create basis as list\n",
    "    basis = []\n",
    "    #hidden dimension is D\n",
    "    for _ in range(param[\"D\"]):\n",
    "        #add one generated vector as a time\n",
    "        basis.append(generate_vector(param[\"nFeatures\"], param[\"vector\"], param))\n",
    "    #make basis numpy array\n",
    "    basis = np.asarray(basis)\n",
    "    return basis\n",
    "\n",
    "\n",
    "\n",
    "        #sys.stderr.write(str(self.basis.shape)+\"\\n\")\n",
    "def updateBasis(basis, param, toChange = None):\n",
    "    print(\"Updating basis......\")# at the following indices: (None means changing everything)\")\n",
    "    #print(toChange)\n",
    "    #For each dimension designated to be dropped\n",
    "    for i in toChange:\n",
    "        #generate a new ith vector in the basis\n",
    "        basis[i] = generate_vector(param[\"nFeatures\"], param[\"vector\"], param)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Config\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from Config import config, Update_T\n",
    "\n",
    "# n = e^-(|x|^2/(2std^2)) <- gaussian function\n",
    "def gauss(x,y,std):\n",
    "  n = np.linalg.norm(x - y)\n",
    "  n = n ** 2\n",
    "  n = n * -1\n",
    "  n = n / (2 * (std**2))\n",
    "  n = np.exp(n)\n",
    "  return n\n",
    "\n",
    "def poly(x,y,c,d):\n",
    "  return (np.dot(x,y) + c) ** d\n",
    "\n",
    "#  dot product/ gauss product/ cos product\n",
    "def kernel(x,y):\n",
    "  dotKernel = np.dot\n",
    "#   gaussKernel = lambda x, y : gauss(x,y,25)\n",
    "#   polyKernel = lambda x,y : poly(x,y,3,5)\n",
    "#   cosKernel = lambda x,y : np.dot(x,y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "  #k = gaussKernel\n",
    "  #k = polyKernel\n",
    "  k = dotKernel\n",
    "  #k = cosKernel\n",
    "  return k(x,y)\n",
    "\n",
    "class HD_classifier:\n",
    "\n",
    "    # Required parameters for the training it supports; will enhance later\n",
    "    options = [\"one_shot\", \"dropout\", \"lr\"]\n",
    "    # required opts for dropout\n",
    "    options_dropout = [\"dropout_rate\", \"update_type\"]\n",
    "\n",
    "    # id: id associated with the basis/encoded data\n",
    "    def __init__(self, D, nClasses, id):\n",
    "        self.D = D\n",
    "        # number of classes\n",
    "        self.nClasses = nClasses\n",
    "        # classes\n",
    "        self.classes = np.zeros((nClasses, D))\n",
    "        self.counts = np.zeros(nClasses)\n",
    "        # If first fit, print out complete configuration\n",
    "        self.first_fit = True\n",
    "        self.id = id\n",
    "        self.idx_weights = np.ones((D))\n",
    "        self.update_cnts = np.zeros((D))\n",
    "        self.mask = np.ones((D))\n",
    "\n",
    "    def getClasses(self):\n",
    "        return self.classes\n",
    "\n",
    "    def update(self, weight, mask, guess, answer, rate, update_type=Update_T.FULL):\n",
    "        sample = weight * mask\n",
    "        self.counts[guess] += 1\n",
    "        self.counts[answer] += 1\n",
    "        # update hypervector weights\n",
    "        if update_type == Update_T.FULL:\n",
    "            self.classes[guess]  -= rate * weight\n",
    "            self.classes[answer] += rate * weight\n",
    "        elif update_type == Update_T.PARTIAL:\n",
    "            self.classes[guess]  -= rate * sample\n",
    "            self.classes[answer] += rate * weight\n",
    "        elif update_type == Update_T.RPARTIAL:\n",
    "            self.classes[guess]  -= rate * weight\n",
    "            self.classes[answer] += rate * sample\n",
    "        elif update_type == Update_T.MASKED:\n",
    "            self.classes[guess]  -= rate * sample\n",
    "            self.classes[answer] += rate * sample\n",
    "        elif update_type == Update_T.HALF:\n",
    "            self.classes[answer] += rate * weight\n",
    "            self.counts[guess] -= 1\n",
    "        elif update_type == Update_T.WEIGHTED:\n",
    "            self.classes[guess]  -= rate * np.multiply(self.idx_weights, sample)\n",
    "            self.classes[answer] += rate * np.multiply(self.idx_weights, sample)\n",
    "        else:\n",
    "            raise Exception(\"unrecognized Update_T\")\n",
    "\n",
    "    # update class vectors with each sample, once\n",
    "    # return train accuracy\n",
    "    def fit(self, data, label, param = None):\n",
    "\n",
    "        assert self.D == data.shape[1]\n",
    "\n",
    "        # Default parameter\n",
    "        if param is None:\n",
    "            #set default configuration\n",
    "            param = Config.config\n",
    "        for option in self.options:\n",
    "            if option not in param:\n",
    "                #if options are missing, set it to default\n",
    "                param[option] = config[option]\n",
    "        #if self.first_fit:\n",
    "        #    sys.stderr.write(\"Fitting with configuration: %s \\n\" % str([(k,param[k]) for k in self.options]))\n",
    "\n",
    "        # Actual fitting\n",
    "\n",
    "        # handling dropout\n",
    "        mask = np.ones(self.D)\n",
    "        if param[\"masked\"]:\n",
    "            mask = np.copy(self.mask)\n",
    "        elif param[\"dropout\"]:\n",
    "            for option in self.options_dropout:\n",
    "                if option not in param:\n",
    "                    param[option] = config[option]\n",
    "            # Mask for dropout\n",
    "            for i in np.random.choice(self.D, int(self.D * (param[\"drop_rate\"])), replace=False):\n",
    "                mask[i] = 0\n",
    "\n",
    "        # fit\n",
    "        r = list(range(data.shape[0]))\n",
    "        random.shuffle(r)\n",
    "        correct = 0\n",
    "        count = 0\n",
    "        for i in r:\n",
    "            sample = data[i] * mask\n",
    "            assert data[i].shape == mask.shape\n",
    "\n",
    "            answer = label[i]\n",
    "            #maxVal = -1\n",
    "            #guess = -1\n",
    "            #for m in range(self.nClasses):\n",
    "            #    val = kernel(self.classes[m], sample)\n",
    "            #    if val > maxVal:\n",
    "            #        maxVal = val\n",
    "            #        guess = m\n",
    "            vals = np.matmul(sample, self.classes.T)\n",
    "            guess = np.argmax(vals)\n",
    "            \n",
    "            if guess != answer:\n",
    "                self.update(data[i], mask, guess, answer, param[\"lr\"], param[\"update_type\"])\n",
    "            else:\n",
    "                correct += 1\n",
    "            count += 1\n",
    "        self.first_fit = False\n",
    "        return correct / count\n",
    "    def predict(self, data):\n",
    "\n",
    "        assert self.D == data.shape[1]\n",
    "\n",
    "        prediction = []\n",
    "        # fit\n",
    "        for i in range(0,data.shape[0]):\n",
    "            maxVal = -1\n",
    "            guess = -1\n",
    "            for m in range(self.nClasses):\n",
    "                val = kernel(self.classes[m], data[i])\n",
    "                if val > maxVal:\n",
    "                    maxVal = val\n",
    "                    guess = m\n",
    "            prediction.append(guess)\n",
    "        return prediction\n",
    "\n",
    "    # given current classifier value, return:\n",
    "    # Variance of each dimension across the classes, and\n",
    "    # The indices in the order from least variance to greatest\n",
    "    def evaluateBasis(self):\n",
    "        #normed_classes = self.classes/(np.sqrt(np.asarray([self.counts])).T)\n",
    "        #variances = np.var(self.classes, axis = 0)\n",
    "        normed_classes = sklearn.preprocessing.normalize(np.asarray(self.classes), norm='l2')\n",
    "        variances = np.var(normed_classes, axis = 0) \n",
    "        assert len(variances) == self.D\n",
    "        order = np.argsort(variances)\n",
    "        return variances, order\n",
    "\n",
    "    # Some basis are to be update\n",
    "    def updateClasses(self, toChange = None):\n",
    "        if toChange is None:\n",
    "            #self.classes = np.zeros((self.nClasses, self.D))\n",
    "            self.classes = sklearn.preprocessing.normalize(np.asarray(self.classes), norm='l2', axis = 0)\n",
    "            self.counts = np.ones(self.nClasses) # An averaged vector is already in\n",
    "        else:\n",
    "            for i in toChange:\n",
    "                self.classes[:,i] = np.zeros(self.nClasses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dump basis and its param into a file, return the name of file\n",
    "def saveEncoded(encoded, labels, id = \"\", data_type = \"unknown\"):\n",
    "    filename = \"encoded_%s_%s.pkl\" % (id, data_type)\n",
    "    sys.stderr.write(\"Dumping data into %s \\n\"%filename)\n",
    "    joblib.dump((encoded, labels), open(filename, \"wb\"), compress=True)\n",
    "    return filename\n",
    "\n",
    "# Load basis from a file\n",
    "def loadEncoded(filename):\n",
    "    encoded, labels = joblib.load(filename)\n",
    "    return encoded, labels\n",
    "\n",
    "# Class: HD_encoder\n",
    "# Use: take in a basis and a noise flag to create instance, call functions to with data to encode\n",
    "class HD_encoder:\n",
    "    def __init__(self, basis, noise=False):\n",
    "        # set basis to encode with\n",
    "        self.basis = basis\n",
    "        # hypervector dimension\n",
    "        self.D = basis.shape[0]\n",
    "        #### Noise not implemented yet\n",
    "        # self.noises = []  \n",
    "        # if noise:\n",
    "        #     self.noises = np.random.uniform(0, 2 * math.pi, self.D)\n",
    "        # else:\n",
    "        #     self.noises = np.zeros(self.D)\n",
    "\n",
    "    #encode one vector/sample into a HD vector\n",
    "    def encodeDatum(self, datum):\n",
    "        #generate outputs\n",
    "        encoded = np.matmul(self.basis, datum)\n",
    "        #nonlinear activation function\n",
    "        encoded = np.cos(encoded)\n",
    "        #return results\n",
    "        return encoded\n",
    "\n",
    "    # encode data using the given basis\n",
    "    # noise: default Gaussian noise\n",
    "    def encodeData(self, data):\n",
    "        # start = time.time()\n",
    "        #no matrix multiplication errors\n",
    "        assert data.shape[1] == self.basis.shape[1]\n",
    "        # noises = []\n",
    "        encoded = []\n",
    "\n",
    "        for i in range(len(data)):\n",
    "            encoded.append(self.encodeDatum(data[i]))\n",
    "        # end = time.time()\n",
    "        #sys.stderr.write(\"Time spent: %d sec\\n\" % int(end - start))\n",
    "        return np.asarray(encoded)\n",
    "\n",
    "    # Replace basis of the HDE\n",
    "    def updateBasis(self, basis):\n",
    "        self.basis = basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class NeuralHD:\n",
    "    def __init__(self, classes : int, features : int, dim : int = 400):\n",
    "        #Configure for hdb, hdc, and hde classes\n",
    "        self.param=Config.config\n",
    "        self.param['nClasses'] = classes\n",
    "        self.param['nFeatures']= features\n",
    "        #hypervector size\n",
    "        self.param['D']=dim\n",
    "        #encoder\n",
    "        self.hde=None\n",
    "        #classifier\n",
    "        self.hdc=None\n",
    "    def __call__(self, x : torch.Tensor):\n",
    "        #True iff the model has been trained\n",
    "        assert self.hde!=None and self.hdc!=None\n",
    "        #return predicted values\n",
    "        return self.predict(x)\n",
    "    def predict(self,x):\n",
    "        #Get hypervectors for all data points\n",
    "        trainencoded=self.hde.encodeData(x)\n",
    "        #return predictions based on similarity to classification hypervectors\n",
    "        return np.array(self.hdc.predict(trainencoded))\n",
    "    def fit(self,traindata, trainlabels,\n",
    "                   epochs,\n",
    "                   regenloops,  # list of effective dimensions to reach \n",
    "                   percentDrop # drop/regen rate \n",
    "                    ):\n",
    "        \n",
    "        # Initialize basis\n",
    "        basis = vanilla(self.param)\n",
    "        # make encoder based on basis\n",
    "        self.hde = HD_encoder(basis)\n",
    "        # find encoded training vectors\n",
    "        trainencoded = self.hde.encodeData(traindata)\n",
    "        # Initialize classification hypervectors\n",
    "        self.hdc = HD_classifier(self.param[\"D\"], self.param[\"nClasses\"], 0)\n",
    "\n",
    "        # calculate amount of dropped dimensions based on percent and original dimension\n",
    "        amountDrop = int(percentDrop * self.hdc.D)#self.param.D?\n",
    "        print(\"Updating times:\", regenloops)\n",
    "\n",
    "        for i in range(regenloops+1): # For each eDs to reach, will checkpoints\n",
    "            print(\"regenloop: \" + str(i))\n",
    "            # train for x epochs\n",
    "            perfect=self.trainreploop(epochs,trainencoded,trainlabels)\n",
    "            #if its the last regeneration training, stop before doing another dimension drop; stop if 100% accuracy\n",
    "            if i==regenloops or perfect:\n",
    "                return #self.hdc,self.hde - unnecessary now that hdc and hde are within a class\n",
    "            print(\"regeneration\")\n",
    "            #do the dimension drop and regeneration\n",
    "            trainencoded=self.regen(basis,amountDrop,traindata)\n",
    "        return \"error\",\"error\"\n",
    "    \n",
    "    def trainreploop(self,epochs,trainencoded,trainlabels):\n",
    "        # Do the train \n",
    "        for j in range(epochs):\n",
    "            # do one pass of training\n",
    "            train_acc = 100 * self.hdc.fit(trainencoded, trainlabels, self.param)\n",
    "            #Test accuracy used to be in here but I took it out because we dont usually know it\n",
    "            print(\"Train: %.2f \\t \\t Test: \"%(train_acc))\n",
    "            # If accuracy is 100, finish\n",
    "            if train_acc == 100:\n",
    "                return True\n",
    "        return False\n",
    "    def regen(self,basis,amountDrop,traindata):\n",
    "        #make order of dimensions according to variaince; also store variances <-- unnecessary?\n",
    "        var, orders = self.hdc.evaluateBasis()\n",
    "        #drop dimensions with lowest variance\n",
    "        toDrop = orders[:amountDrop]\n",
    "        # print(\"Variances stats: max %.2f, min %.2f, mean %.2f\"%(max(var),min(var),np.mean(var)))\n",
    "        #update basis by randomizing dimension\n",
    "        updateBasis(basis,self.param,toDrop)\n",
    "        # move the new basis into the encoder\n",
    "        self.hde.updateBasis(basis)\n",
    "        # normalize previous classes so retrain has enough effect\n",
    "        self.hdc.updateClasses()\n",
    "        # get new encoded training data\n",
    "        return self.hde.encodeData(traindata)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating times: 5\n",
      "regenloop: 0\n",
      "Train: 98.36 \t \t Test: \n",
      "Train: 98.66 \t \t Test: \n",
      "regeneration\n",
      "Updating basis......\n",
      "regenloop: 1\n",
      "Train: 98.55 \t \t Test: \n",
      "Train: 98.72 \t \t Test: \n",
      "regeneration\n",
      "Updating basis......\n",
      "regenloop: 2\n",
      "Train: 98.55 \t \t Test: \n",
      "Train: 98.72 \t \t Test: \n",
      "regeneration\n",
      "Updating basis......\n",
      "regenloop: 3\n",
      "Train: 98.56 \t \t Test: \n",
      "Train: 98.72 \t \t Test: \n",
      "regeneration\n",
      "Updating basis......\n",
      "regenloop: 4\n",
      "Train: 98.60 \t \t Test: \n",
      "Train: 98.76 \t \t Test: \n",
      "regeneration\n",
      "Updating basis......\n",
      "regenloop: 5\n",
      "Train: 98.60 \t \t Test: \n",
      "Train: 98.77 \t \t Test: \n"
     ]
    }
   ],
   "source": [
    "model=NeuralHD(5,xtrain.shape[1],300)\n",
    "model.fit(xtrain,ytrain,2,5,.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0,model.param['nClasses']):\n",
    "    yhat= model(x_test[y_test==i])\n",
    "    acc = (yhat==i).mean()\n",
    "    print('class '+str(i)+' accuracy: ' +str(acc))\n",
    "    print('points: '+ str(len(yhat)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
