{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ekans\n"
     ]
    }
   ],
   "source": [
    "print(\"ekans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onlinehd as Onlinehd\n",
    "import warnings\n",
    "\n",
    "from time import time\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "import torch\n",
    "import Config\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetname=\"Microsoft Challenge BIG 2015\"\n",
    "datasets=[\n",
    "    \"KDD Cup 1999\",                            #0\n",
    "    \"Microsoft Challenge BIG 2015\"             #1\n",
    "]\n",
    "presets = {\n",
    "    \"KDD Cup 1999\": {\n",
    "        \"NeuralHD\": [300,2,3,.1],\n",
    "        \"OnlineHD\": [300,1.0,.1,30,True],\n",
    "        \"MLP\": [100,5,.001],\n",
    "        \"SVM\": [10000]\n",
    "    },\n",
    "    \"Microsoft Challenge BIG 2015\": {\n",
    "        \"NeuralHD\": [3000,6,10,.1],\n",
    "        \"OnlineHD\": [3000,1.0,.1,30,True],\n",
    "        \"MLP\": [100,30,.001],\n",
    "        \"SVM\": [None]\n",
    "    }\n",
    "}\n",
    "def normalized(x,y):\n",
    "    xtrain, x_test, ytrain, y_test = None,None,None,None\n",
    "    x, x_test, y, y_test = sklearn.model_selection.train_test_split(x, y, shuffle=True)\n",
    "    scaler = sklearn.preprocessing.Normalizer().fit(x)\n",
    "    x = scaler.transform(x)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    # changes data to pytorch's tensors\n",
    "    x = torch.from_numpy(x).float()\n",
    "    y = torch.from_numpy(y).long()\n",
    "    x_test = torch.from_numpy(x_test).float()\n",
    "    y_test = torch.from_numpy(y_test).long()\n",
    "    return x.numpy(), x_test.numpy(), y.numpy(), y_test.numpy(), scaler\n",
    "def getuniquevalues(columnname,df):\n",
    "    values={}\n",
    "    i=0\n",
    "    for entry in df[columnname]:\n",
    "        if entry not in values:\n",
    "            values[entry]=i\n",
    "            i+=1\n",
    "    return values\n",
    "def get_dataset(name):\n",
    "    if name==datasets[0]:\n",
    "        path=\"../../Data/\"\n",
    "        attacks_types = {\n",
    "            'normal': 'normal','back': 'dos','buffer_overflow': 'u2r','ftp_write': 'r2l','guess_passwd': 'r2l',\n",
    "        'imap': 'r2l','ipsweep': 'probe','land': 'dos','loadmodule': 'u2r','multihop': 'r2l','neptune': 'dos',\n",
    "        'nmap': 'probe','perl': 'u2r','phf': 'r2l','pod': 'dos','portsweep': 'probe','rootkit': 'u2r','satan': 'probe',\n",
    "        'smurf': 'dos','spy': 'r2l','teardrop': 'dos','warezclient': 'r2l','warezmaster': 'r2l',\n",
    "        }\n",
    "        cols =\"\"\"duration,protocol_type,service,flag,src_bytes,dst_bytes,land,wrong_fragment,\n",
    "        urgent,hot,num_failed_logins,logged_in,num_compromised,root_shell,su_attempted,num_root,\n",
    "        num_file_creations,num_shells,num_access_files,num_outbound_cmds,is_host_login,is_guest_login,\n",
    "        count,srv_count,serror_rate,srv_serror_rate,rerror_rate,srv_rerror_rate,same_srv_rate,\n",
    "        diff_srv_rate,srv_diff_host_rate,dst_host_count,dst_host_srv_count,dst_host_same_srv_rate,\n",
    "        dst_host_diff_srv_rate,dst_host_same_src_port_rate,dst_host_srv_diff_host_rate,\n",
    "        dst_host_serror_rate,dst_host_srv_serror_rate,dst_host_rerror_rate,dst_host_srv_rerror_rate\"\"\"\n",
    "        \n",
    "        columns =[]\n",
    "        for c in cols.split(','):\n",
    "            if(c.strip()):\n",
    "                columns.append(c.strip())\n",
    "        print(len(columns))\n",
    "        columns.append('target')\n",
    "        print(len(columns))\n",
    "\n",
    "        attack_categories=[\"dos\",\"u2r\",\"r2l\",'probe','normal']\n",
    "        df = pd.read_csv(path+\"kddcup.data_10_percent.gz\", names = columns)\n",
    "        df['Attack Type'] = df.target.apply(lambda r:attacks_types[r[:-1]])\n",
    "        del df['target']\n",
    "        df.head()\n",
    "        num_cols = df._get_numeric_data().columns\n",
    "        \n",
    "        cate_cols = list(set(df.columns)-set(num_cols))\n",
    "        cate_cols.remove('Attack Type')\n",
    "        for col in cate_cols:\n",
    "            df[col]=df[col].map(getuniquevalues(col,df))\n",
    "        data=df.to_numpy()\n",
    "        Y=df['Attack Type'].map(getuniquevalues('Attack Type',df))\n",
    "        Y=Y.to_numpy()\n",
    "        X=data[:,:-1]\n",
    "        print(Y.shape)\n",
    "        print(X.shape)\n",
    "        print(getuniquevalues('Attack Type',df))\n",
    "        xtrain, x_test, ytrain, y_test,scaler= normalized(X,Y)\n",
    "    if name==datasets[1]:\n",
    "        path=\"../../Data/malware-classification/\"\n",
    "        map={}\n",
    "        mapping=pd.read_csv(path + \"trainLabels.csv\")\n",
    "        Y=mapping[\"Class\"].to_numpy()\n",
    "        for i in range(0,len(Y)):\n",
    "            map[mapping[\"Id\"][i]]=mapping[\"Class\"][i]-1\n",
    "        byte_features=pd.read_csv(path+\"result.csv\")\n",
    "        byte_features['ID']  = byte_features['ID'].str.split('.').str[0]\n",
    "        byte_features.head(3)\n",
    "        byte_features['ID']=byte_features['ID'].map(map)\n",
    "        data=byte_features.to_numpy()\n",
    "        X=data[:,1:]\n",
    "        Y=data[:,0]\n",
    "        xtrain, x_test, ytrain, y_test,scaler= normalized(X,Y)\n",
    "    return xtrain,x_test,ytrain,y_test\n",
    "\n",
    "xtrain,x_test,ytrain,y_test=get_dataset(datasetname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtraintorch=torch.from_numpy(xtrain)\n",
    "ytraintorch=torch.from_numpy(ytrain)\n",
    "x_testtorch=torch.from_numpy(x_test)\n",
    "y_testtorch=torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_accuracy_breakdown(model,x_test,y_test, output=False):\n",
    "    acc=[]\n",
    "    points=[]\n",
    "    try:\n",
    "        for i in range (0,len(np.unique(y_test))):\n",
    "            yhat= model.predict(x_test[y_test==i])\n",
    "            if len(yhat.shape)==2:\n",
    "                yhat=np.array([row.argmax() for row in yhat])\n",
    "            acc.append((yhat==i).mean())\n",
    "            points.append(len(yhat))\n",
    "    except:\n",
    "        for i in range (0,len(np.unique(y_test))):\n",
    "            yhat= model.predict(torch.from_numpy(x_test[y_test==i]))\n",
    "            acc.append((yhat==i).float().mean())\n",
    "            points.append(len(yhat))\n",
    "    # print(yhat[:30])\n",
    "    totacc=sum([acc[i]*points[i] for i in range(0,len(acc))])/sum(points)\n",
    "    if output:\n",
    "        plt.bar(range(0,len(acc)),acc,color=np.random.rand(3,))\n",
    "        plt.title(\"Accuracy Total: \" + str(totacc))\n",
    "        plt.show()\n",
    "    return totacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change\n",
    "def cos_cdist(x1 : torch.Tensor, x2 : torch.Tensor, eps : float = 1e-8):\n",
    "    #Cosine Similarity\n",
    "    eps = torch.tensor(eps, device=x1.device)\n",
    "    norms1 = x1.norm(dim=1).unsqueeze_(1).max(eps)\n",
    "    norms2 = x2.norm(dim=1).unsqueeze_(0).max(eps)\n",
    "    cdist = x1 @ x2.T\n",
    "    cdist.div_(norms1).div_(norms2)\n",
    "    return cdist\n",
    "class NeuralHD:\n",
    "    def __init__(self, classes : int, features : int, dim : int = 400, batch_size=1,trainopt=3,bestinclass=False,lr=.003):\n",
    "        #Configure for hdb, hdc, and hde classes\n",
    "        print(\"test\")\n",
    "        self.param=Config.config\n",
    "        self.param['nClasses'] = classes\n",
    "        self.param['nFeatures']= features\n",
    "        #hypervector size\n",
    "        self.param['D']=dim\n",
    "        self.param['lr']=.0001\n",
    "        self.batch_size=batch_size\n",
    "        self.base = torch.empty(self.param['D']).uniform_(0.0, 2*math.pi)\n",
    "\n",
    "        self.bestinclass=bestinclass\n",
    "        #encoder\n",
    "        self.hde=None\n",
    "        #classifier\n",
    "        self.hdc=None\n",
    "        # Initialize basis in gaussian distribution\n",
    "        self.basis = torch.normal(0,1,size=(self.param[\"D\"],self.param[\"nFeatures\"]))\n",
    "        # Initialize classification hypervectors\n",
    "        self.classes = torch.zeros((self.param['nClasses'], self.param['D']))\n",
    "        self.prevacc=0\n",
    "        self.trainoption=trainopt\n",
    "        self.trainfunctions=[self.train,self.train2,self.train3]\n",
    "        # self.param['lr']=.1\n",
    "        # self.hdc = HD_classifier(self.param[\"D\"], self.param[\"nClasses\"], 0)\n",
    "        self.trainaccuracies=[]\n",
    "        self.testaccuracies=[]\n",
    "        self.medians=[]\n",
    "    def __call__(self, x : torch.Tensor):\n",
    "        #return predicted values\n",
    "        return self.predict(x)\n",
    "    def encode(self,x):\n",
    "        n = x.size(0)\n",
    "        bsize = min([x.size(1),1024])\n",
    "        h = torch.empty(n, self.basis.shape[0], device=x.device, dtype=x.dtype)\n",
    "        temp = torch.empty(bsize, self.basis.shape[0], device=x.device, dtype=x.dtype)\n",
    "\n",
    "        # we need batches to remove memory usage\n",
    "        for i in range(0, n, bsize):\n",
    "            torch.matmul(x[i:i+bsize], self.basis.T, out=temp)\n",
    "\n",
    "            # self.noise ... I haven't seen any indication that it works better \n",
    "            # if self.noise:\n",
    "            torch.add(temp, self.base, out=h[i:i+bsize])#h[i:i+bsize]=temp# torch.add(temp, self.base, out=h[i:i+bsize])\n",
    "            # else:\n",
    "            # h[i:i+bsize]=temp\n",
    "\n",
    "            h[i:i+bsize].cos_()#.mul_(temp.sin_())\n",
    "        # print(h.shape)\n",
    "        return h\n",
    "    def train(self,h,y):\n",
    "        print(\"1\")\n",
    "        # r=torch.randperm(y.size(0))\n",
    "        # y=y[r]\n",
    "        # h=h[r,:]\n",
    "        n = h.size(0)\n",
    "        batch_size = min([y.size(0), self.batch_size])#64\n",
    "        for i in range(0, n, batch_size):\n",
    "            h_ = h[i:i+batch_size]\n",
    "            y_ = y[i:i+batch_size]\n",
    "            scores = cos_cdist(h_, self.classes)#cos\n",
    "            y_pred = scores.argmax(1)\n",
    "            wrong = y_ != y_pred\n",
    "\n",
    "            # computes alphas to update model\n",
    "            # alpha1 = 1 - delta[lbl] -- the true label coefs\n",
    "            # alpha2 = delta[max] - 1 -- the prediction coefs\n",
    "            aranged = torch.arange(h_.size(0), device=h_.device)\n",
    "            alpha1 = (1.0 - scores[aranged,y_]).unsqueeze_(1)\n",
    "            alpha2 = (scores[aranged,y_pred] - 1.0).unsqueeze_(1)\n",
    "\n",
    "            for lbl in y_.unique():\n",
    "                m1 = wrong & (y_ == lbl) # mask of missed true lbl\n",
    "                m2 = wrong & (y_pred == lbl) # mask of wrong preds\n",
    "                self.classes[lbl] += self.param['lr']*(alpha1[m1]*h_[m1]).sum(0)\n",
    "                self.classes[lbl] += self.param['lr']*(alpha2[m2]*h_[m2]).sum(0)\n",
    "            # if self.test(h,y)<self.prevacc:\n",
    "            #     for lbl in y_.unique():\n",
    "            #         m1 = wrong & (y_ == lbl) # mask of missed true lbl\n",
    "            #         m2 = wrong & (y_pred == lbl) # mask of wrong preds\n",
    "            #         self.classes[lbl] -= self.param['lr']*(alpha1[m1]*h_[m1]).sum(0)\n",
    "            #         self.classes[lbl] -= self.param['lr']*(alpha2[m2]*h_[m2]).sum(0)\n",
    "            # else:\n",
    "            #     self.prevacc=self.test(h,y)\n",
    "    def train2(self,h,y):\n",
    "        # def fit(self, data, label, param = None):\n",
    "        print(\"2\")\n",
    "        assert self.param[\"D\"] == h.size(1)\n",
    "        #if self.first_fit:\n",
    "        #    sys.stderr.write(\"Fitting with configuration: %s \\n\" % str([(k,param[k]) for k in self.options]))\n",
    "\n",
    "        # Actual fitting\n",
    "\n",
    "        # handling dropout\n",
    "\n",
    "        # fit\n",
    "        r = torch.randperm(h.shape[0])\n",
    "        correct = 0\n",
    "        count = 0\n",
    "        for i in r:\n",
    "            sample = h[i] \n",
    "            answer = y[i]\n",
    "            #maxVal = -1\n",
    "            #guess = -1\n",
    "            #for m in range(self.nClasses):\n",
    "            #    val = kernel(self.classes[m], sample)\n",
    "            #    if val > maxVal:\n",
    "            #        maxVal = val\n",
    "            #        guess = m\n",
    "            vals = cos_cdist(sample.unsqueeze(1).T, self.classes)\n",
    "            # print(vals)\n",
    "            guess = torch.argmax(vals)\n",
    "            if guess != answer:\n",
    "                self.classes[guess]-=self.param['lr']*h[i]*(1-vals[0,guess])\n",
    "                self.classes[answer]+=self.param['lr']*h[i]*(1-vals[0,answer])\n",
    "                # acc=self.test2(h[r][:100],y)\n",
    "                # if acc<=self.prevacc:\n",
    "                #     self.classes[guess]+=self.param['lr']*h[i]\n",
    "                #     self.classes[answer]-=self.param['lr']*h[i]\n",
    "                # else:\n",
    "                #     self.prevacc=acc\n",
    "            else:\n",
    "                correct += 1\n",
    "            count += 1\n",
    "        return correct / count\n",
    "    \n",
    "    def train3(self,h,y):\n",
    "        # def fit(self, data, label, param = None):\n",
    "        print(\"3\")\n",
    "        assert self.param[\"D\"] == h.size(1)\n",
    "        #if self.first_fit:\n",
    "        #    sys.stderr.write(\"Fitting with configuration: %s \\n\" % str([(k,param[k]) for k in self.options]))\n",
    "\n",
    "        # Actual fitting\n",
    "\n",
    "        # handling dropout\n",
    "\n",
    "        # fit\n",
    "        r = torch.randperm(y.size(0))\n",
    "        y=y[r]\n",
    "        h=h[r,:]\n",
    "        correct = 0\n",
    "        count = 0\n",
    "        for i in range(0,y.size(0),self.batch_size):\n",
    "            sample = h[i:i+self.batch_size] \n",
    "            answers = y[i:i+self.batch_size]\n",
    "            #maxVal = -1\n",
    "            #guess = -1\n",
    "            #for m in range(self.nClasses):\n",
    "            #    val = kernel(self.classes[m], sample)\n",
    "            #    if val > maxVal:\n",
    "            #        maxVal = val\n",
    "            #        guess = m\n",
    "            vals = cos_cdist(sample, self.classes)\n",
    "            # print(vals)\n",
    "            guesses = vals.argmax(1)\n",
    "            # print(guesses)\n",
    "            for j in range(0,answers.size(0)):\n",
    "                if guesses[j] != answers[j]:\n",
    "                    # print(answers[j])\n",
    "                    self.classes[guesses[j]]-=self.param['lr']*h[i+j]*(1-vals[0,guesses[j]])\n",
    "                    self.classes[answers[j]]+=self.param['lr']*h[i+j]*(1-vals[0,answers[j]])\n",
    "                    # acc=self.test2(h[r][:100],y)\n",
    "                    # if acc<=self.prevacc:\n",
    "                    #     self.classes[guess]+=self.param['lr']*h[i]\n",
    "                    #     self.classes[answer]-=self.param['lr']*h[i]\n",
    "                    # else:\n",
    "                    #     self.prevacc=acc\n",
    "                else:\n",
    "                    correct += 1\n",
    "                count += 1\n",
    "        return correct / count\n",
    "\n",
    "    def predict(self,x):\n",
    "        #return predictions based on similarity of encoded inputs to classification hypervectors\n",
    "        return  cos_cdist(self.encode(x), self.classes).argmax(1)\n",
    "    def fit(self,traindata, trainlabels,\n",
    "                   epochs,\n",
    "                   regenloops,  # list of effective dimensions to reach \n",
    "                   fractionToDrop # drop/regen rate \n",
    "                    ):\n",
    "        # find encoded training vectors\n",
    "\n",
    "        # calculate amount of dropped dimensions based on percent and original dimension\n",
    "        amountDrop = int(fractionToDrop * self.param['D'])#self.param.D?\n",
    "        # print(\"Updating times:\", regenloops)\n",
    "\n",
    "        for i in range(regenloops+1): # For each eDs to reach, will checkpoints\n",
    "            # compute new encoded data\n",
    "            trainencoded = self.encode(traindata)\n",
    "            testencoded = self.encode(x_testtorch)\n",
    "            \n",
    "            # print(\"regenloop: \" + str(i))\n",
    "            # train for specified number of epochs\n",
    "            # Do the train \n",
    "            self.prevacc=0\n",
    "            iterscorestrain=[]\n",
    "            iterscorestest=[]\n",
    "            maxval=0\n",
    "            temp=None\n",
    "            for j in range(epochs):\n",
    "                # do one pass of training\n",
    "                # print(self.classes[:,8])\n",
    "                result=self.trainfunctions[self.trainoption](trainencoded, trainlabels)\n",
    "                trainaccuracy= self.test(trainencoded,trainlabels)\n",
    "                testaccuracy= self.test(testencoded,y_testtorch)\n",
    "                print(testaccuracy)\n",
    "                iterscorestrain.append(trainaccuracy)\n",
    "                iterscorestest.append(testaccuracy)\n",
    "\n",
    "                if self.bestinclass and trainaccuracy>maxval:\n",
    "                    temp=copy.deepcopy(self.classes)\n",
    "                    maxval=trainaccuracy\n",
    "                    print(testaccuracy)\n",
    "                # print(j)\n",
    "            if self.bestinclass:\n",
    "                self.classes=temp\n",
    "            \n",
    "            self.trainaccuracies+=iterscorestrain\n",
    "            self.testaccuracies+=iterscorestest\n",
    "            self.medians.append(np.median(np.array(iterscorestrain)))\n",
    "                # print(self.prevacc)\n",
    "            #if its the last regeneration training, stop before doing another dimension drop; stop if 100% accuracy\n",
    "            if i==regenloops:\n",
    "                return #self.hdc,self.hde - unnecessary now that hdc and hde are within a class\n",
    "            # print(\"regen\" +str(i))\n",
    "            #do the dimension drop and regeneration\n",
    "            normed_classes = torch.nn.functional.normalize(self.classes)\n",
    "            #calculate variances for each dimension\n",
    "            var = torch.var(normed_classes, 0) \n",
    "            assert len(var) == self.param['D']\n",
    "            # rank each entry in variances from smallest to largest\n",
    "            order = torch.argsort(var)\n",
    "            #drop amountDrop bases\n",
    "            toDrop = order[:amountDrop]\n",
    "            #            ----------------\n",
    "            #attempted reverse drop\n",
    "            # if amountDrop<0:\n",
    "            #     toDrop = order[-amountDrop:]\n",
    "            #            ----------------\n",
    "            #Update basis\n",
    "            #For each dimension designated to be dropped\n",
    "            for i in toDrop:\n",
    "                #generate a new ith vector in the basis\n",
    "                self.basis[i] = torch.normal(self.param[\"mu\"],self.param[\"sigma\"], size=(self.param['nFeatures'],))\n",
    "            #Update Classes\n",
    "            #            --------------\n",
    "            #This code was left out. Maybe useful?\n",
    "            for i in toDrop:\n",
    "                self.classes[:,i] = torch.zeros(self.param['nClasses'])\n",
    "            #            --------------\n",
    "\n",
    "            self.classes=torch.nn.functional.normalize(self.classes)\n",
    "            # self.batch_size=int(np.ceil(self.batch_size/2))\n",
    "            # if self.batch_size==1:\n",
    "            #     self.param['lr']=self.param['lr']/2\n",
    "        return \"error\",\"error\"\n",
    "    def _iterative_fit(self, h, y, lr, epochs):\n",
    "        h=self.encode(h)\n",
    "        n = h.size(0)\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(0, n, self.batch_size):\n",
    "                h_ = h[i:i+self.batch_size]\n",
    "                y_ = y[i:i+self.batch_size]\n",
    "                scores = cos_cdist(h_, self.classes)\n",
    "                y_pred = scores.argmax(1)\n",
    "                wrong = y_ != y_pred\n",
    "\n",
    "                # computes alphas to update model\n",
    "                # alpha1 = 1 - delta[lbl] -- the true label coefs\n",
    "                # alpha2 = delta[max] - 1 -- the prediction coefs\n",
    "                aranged = torch.arange(h_.size(0), device=h_.device)\n",
    "                alpha1 = (1.0 - scores[aranged,y_]).unsqueeze_(1)\n",
    "                alpha2 = (scores[aranged,y_pred] - 1.0).unsqueeze_(1)\n",
    "\n",
    "                for lbl in y_.unique():\n",
    "                    m1 = wrong & (y_ == lbl) # mask of missed true lbl\n",
    "                    m2 = wrong & (y_pred == lbl) # mask of wrong preds\n",
    "                    self.classes[lbl] += lr*(alpha1[m1]*h_[m1]).sum(0)\n",
    "                    self.classes[lbl] += lr*(alpha2[m2]*h_[m2]).sum(0)\n",
    "    def test(self,x_encoded, y_labels):\n",
    "            yhat= cos_cdist(x_encoded, self.classes).argmax(1)\n",
    "            return (yhat==y_labels).float().mean()\n",
    "    def test2(self,x_encoded,y_labels):\n",
    "        yhat=torch.zeros(y_labels.size(0))\n",
    "        i=0\n",
    "        for v in x_encoded:\n",
    "            sims=torch.matmul(v,self.classes.T)\n",
    "            yhat[i]=torch.argmax(sims)\n",
    "            i+=1\n",
    "        return (yhat==y_labels).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-b2219913e319>:51: UserWarning: An output with one or more elements was resized since it had shape [257, 1024], which does not match the required output shape [184, 1024].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)\n",
      "  torch.matmul(x[i:i+bsize], self.basis.T, out=temp)\n",
      "<ipython-input-28-b2219913e319>:51: UserWarning: An output with one or more elements was resized since it had shape [257, 1024], which does not match the required output shape [147, 1024].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)\n",
      "  torch.matmul(x[i:i+bsize], self.basis.T, out=temp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8616)\n",
      "3\n",
      "tensor(0.8874)\n",
      "3\n",
      "tensor(0.9073)\n",
      "3\n",
      "tensor(0.8495)\n",
      "3\n",
      "tensor(0.9006)\n",
      "3\n",
      "tensor(0.9003)\n",
      "3\n",
      "tensor(0.9080)\n",
      "3\n",
      "tensor(0.9139)\n",
      "3\n",
      "tensor(0.8877)\n",
      "3\n",
      "tensor(0.9253)\n",
      "3\n",
      "tensor(0.9323)\n",
      "3\n",
      "tensor(0.8947)\n",
      "3\n",
      "tensor(0.9238)\n",
      "3\n",
      "tensor(0.9404)\n",
      "3\n",
      "tensor(0.9187)\n",
      "3\n",
      "tensor(0.9507)\n",
      "3\n",
      "tensor(0.9503)\n",
      "3\n",
      "tensor(0.9551)\n",
      "3\n",
      "tensor(0.9507)\n",
      "3\n",
      "tensor(0.9525)\n",
      "3\n",
      "tensor(0.9525)\n",
      "3\n",
      "tensor(0.9547)\n",
      "3\n",
      "tensor(0.9573)\n",
      "3\n",
      "tensor(0.9544)\n",
      "3\n",
      "tensor(0.9555)\n",
      "3\n",
      "tensor(0.9544)\n",
      "3\n",
      "tensor(0.9529)\n",
      "3\n",
      "tensor(0.9547)\n",
      "3\n",
      "tensor(0.9499)\n",
      "3\n",
      "tensor(0.9525)\n",
      "3\n",
      "tensor(0.9551)\n",
      "3\n",
      "tensor(0.9566)\n",
      "3\n",
      "tensor(0.9544)\n",
      "3\n",
      "tensor(0.9566)\n",
      "3\n",
      "tensor(0.9566)\n",
      "3\n",
      "tensor(0.9566)\n",
      "3\n",
      "tensor(0.9544)\n",
      "3\n",
      "tensor(0.9492)\n",
      "3\n",
      "tensor(0.9573)\n",
      "3\n",
      "tensor(0.9551)\n",
      "3\n",
      "tensor(0.9547)\n",
      "3\n",
      "tensor(0.9551)\n",
      "3\n",
      "tensor(0.9540)\n",
      "3\n",
      "tensor(0.9577)\n",
      "3\n",
      "tensor(0.9562)\n",
      "3\n",
      "tensor(0.9569)\n",
      "3\n",
      "tensor(0.9562)\n",
      "3\n",
      "tensor(0.9551)\n",
      "3\n",
      "tensor(0.9558)\n",
      "3\n",
      "tensor(0.9577)\n",
      "3\n",
      "tensor(0.9562)\n",
      "3\n",
      "tensor(0.9514)\n",
      "3\n",
      "tensor(0.9577)\n",
      "3\n",
      "tensor(0.9551)\n",
      "3\n",
      "tensor(0.9569)\n",
      "3\n",
      "tensor(0.9577)\n",
      "3\n",
      "tensor(0.9558)\n",
      "3\n",
      "tensor(0.9580)\n",
      "3\n",
      "tensor(0.9566)\n",
      "3\n",
      "tensor(0.9529)\n",
      "3\n",
      "tensor(0.9547)\n",
      "3\n",
      "tensor(0.9536)\n",
      "3\n",
      "tensor(0.9566)\n",
      "3\n",
      "tensor(0.9547)\n",
      "3\n",
      "tensor(0.9566)\n",
      "3\n",
      "tensor(0.9566)\n",
      "3\n",
      "tensor(0.9558)\n",
      "3\n",
      "tensor(0.9562)\n",
      "3\n",
      "tensor(0.9566)\n",
      "3\n",
      "tensor(0.9569)\n",
      "3\n",
      "tensor(0.9588)\n",
      "3\n",
      "tensor(0.9522)\n",
      "3\n",
      "tensor(0.9551)\n",
      "3\n",
      "tensor(0.9584)\n",
      "3\n",
      "tensor(0.9566)\n"
     ]
    }
   ],
   "source": [
    "model=NeuralHD(len(np.unique(y_test)),xtrain.shape[1],1024,batch_size=1,trainopt=2,bestinclass=False)\n",
    "model.fit(xtraintorch,ytraintorch,15,4,.1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
