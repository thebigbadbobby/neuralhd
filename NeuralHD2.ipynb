{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import time\n",
    "import torch\n",
    "import sklearn.datasets\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.datasets import FashionMNIST as FMNIST\n",
    "from torchvision.datasets import EMNIST\n",
    "import torchvision.transforms as transforms\n",
    "import tensorflow as tf\n",
    "import random as r\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam, SGD, Adagrad, Adadelta, RMSprop\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Activation, BatchNormalization\n",
    "import sklearn\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import joblib\n",
    "from tqdm import tqdm_notebook\n",
    "import copy\n",
    "\n",
    "import Config\n",
    "import Dataloader as DL\n",
    "import HD_basis as HDB\n",
    "import HD_encoder as HDE\n",
    "import HD_classifier as HDC\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "42\n",
      "(494021,)\n",
      "(494021, 41)\n",
      "{'normal': 0, 'u2r': 1, 'dos': 2, 'r2l': 3, 'probe': 4}\n"
     ]
    }
   ],
   "source": [
    "path=\"../../Data/\"\n",
    "attacks_types = {\n",
    "    'normal': 'normal','back': 'dos','buffer_overflow': 'u2r','ftp_write': 'r2l','guess_passwd': 'r2l',\n",
    "'imap': 'r2l','ipsweep': 'probe','land': 'dos','loadmodule': 'u2r','multihop': 'r2l','neptune': 'dos',\n",
    "'nmap': 'probe','perl': 'u2r','phf': 'r2l','pod': 'dos','portsweep': 'probe','rootkit': 'u2r','satan': 'probe',\n",
    "'smurf': 'dos','spy': 'r2l','teardrop': 'dos','warezclient': 'r2l','warezmaster': 'r2l',\n",
    "}\n",
    "cols =\"\"\"duration,protocol_type,service,flag,src_bytes,dst_bytes,land,wrong_fragment,\n",
    "urgent,hot,num_failed_logins,logged_in,num_compromised,root_shell,su_attempted,num_root,\n",
    "num_file_creations,num_shells,num_access_files,num_outbound_cmds,is_host_login,is_guest_login,\n",
    "count,srv_count,serror_rate,srv_serror_rate,rerror_rate,srv_rerror_rate,same_srv_rate,\n",
    "diff_srv_rate,srv_diff_host_rate,dst_host_count,dst_host_srv_count,dst_host_same_srv_rate,\n",
    "dst_host_diff_srv_rate,dst_host_same_src_port_rate,dst_host_srv_diff_host_rate,\n",
    "dst_host_serror_rate,dst_host_srv_serror_rate,dst_host_rerror_rate,dst_host_srv_rerror_rate\"\"\"\n",
    "  \n",
    "columns =[]\n",
    "for c in cols.split(','):\n",
    "    if(c.strip()):\n",
    "       columns.append(c.strip())\n",
    "print(len(columns))\n",
    "columns.append('target')\n",
    "print(len(columns))\n",
    "\n",
    "attack_categories=[\"dos\",\"u2r\",\"r2l\",'probe','normal']\n",
    "df = pd.read_csv(path+\"kddcup.data_10_percent.gz\", names = columns)\n",
    "df['Attack Type'] = df.target.apply(lambda r:attacks_types[r[:-1]])\n",
    "del df['target']\n",
    "df.head()\n",
    "num_cols = df._get_numeric_data().columns\n",
    "  \n",
    "cate_cols = list(set(df.columns)-set(num_cols))\n",
    "cate_cols.remove('Attack Type')\n",
    "def getuniquevalues(columnname):\n",
    "    values={}\n",
    "    i=0\n",
    "    for entry in df[columnname]:\n",
    "        if entry not in values:\n",
    "            values[entry]=i\n",
    "            i+=1\n",
    "    return values\n",
    "for col in cate_cols:\n",
    "    df[col]=df[col].map(getuniquevalues(col))\n",
    "data=df.to_numpy()\n",
    "Y=df['Attack Type'].map(getuniquevalues('Attack Type'))\n",
    "Y=Y.to_numpy()\n",
    "X=data[:,:-1]\n",
    "print(Y.shape)\n",
    "print(X.shape)\n",
    "print(getuniquevalues('Attack Type'))\n",
    "def normalized(x,y):\n",
    "    x, x_test, y, y_test = sklearn.model_selection.train_test_split(x, y, shuffle=True)\n",
    "    scaler = sklearn.preprocessing.Normalizer().fit(x)\n",
    "    x = scaler.transform(x)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    # changes data to pytorch's tensors\n",
    "    x = torch.from_numpy(x).float()\n",
    "    y = torch.from_numpy(y).long()\n",
    "    x_test = torch.from_numpy(x_test).float()\n",
    "    y_test = torch.from_numpy(y_test).long()\n",
    "    return x.numpy(), x_test.numpy(), y.numpy(), y_test.numpy(), scaler\n",
    "xtrain, x_test, ytrain, y_test,scaler= normalized(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralHD:\n",
    "    def __init__(self, classes : int, features : int, dim : int = 400):\n",
    "        #Configure for hdb, hdc, and hde classes\n",
    "        self.param=Config.config\n",
    "        self.param['nClasses'] = classes\n",
    "        self.param['nFeatures']= features\n",
    "        #hypervector size\n",
    "        self.param['D']=dim\n",
    "        #encoder\n",
    "        self.hde=None\n",
    "        #classifier\n",
    "        self.hdc=None\n",
    "    def __call__(self, x : torch.Tensor):\n",
    "        #True iff the model has been trained\n",
    "        assert self.hde!=None and self.hdc!=None\n",
    "        #return predicted values\n",
    "        return self.predict(x)\n",
    "    def predict(self,x):\n",
    "        #Get hypervectors for all data points\n",
    "        trainencoded=self.hde.encodeData(x)\n",
    "        #return predictions based on similarity to classification hypervectors\n",
    "        return np.array(self.hdc.predict(trainencoded))\n",
    "    def fit(self,traindata, trainlabels,\n",
    "                   epochs,\n",
    "                   regenloops,  # list of effective dimensions to reach \n",
    "                   percentDrop # drop/regen rate \n",
    "                    ):\n",
    "        \n",
    "        # Initialize basis & classifier\n",
    "        hdb = HDB.HD_basis(HDB.Generator.Vanilla, self.param)\n",
    "        # store generated basis\n",
    "        basis = hdb.getBasis()\n",
    "        # updata params after basis generation\n",
    "        self.param = hdb.getParam()\n",
    "        # make encoder based on basis\n",
    "        self.hde = HDE.HD_encoder(basis)\n",
    "        # find encoded training vectors\n",
    "        trainencoded = self.hde.encodeData(traindata)\n",
    "        # Initialize classification hypervectors\n",
    "        self.hdc = HDC.HD_classifier(self.param[\"D\"], self.param[\"nClasses\"], 0)\n",
    "\n",
    "        # calculate amount of dropped dimensions based on percent and original dimension\n",
    "        amountDrop = int(percentDrop * self.hdc.D)#self.param.D?\n",
    "        print(\"Updating times:\", regenloops)\n",
    "\n",
    "        for i in range(regenloops+1): # For each eDs to reach, will checkpoints\n",
    "            print(\"regenloop: \" + str(i))\n",
    "            # train for x epochs\n",
    "            perfect=self.trainreploop(epochs,trainencoded,trainlabels)\n",
    "            #if its the last regeneration training, stop before doing another dimension drop; stop if 100% accuracy\n",
    "            if i==regenloops or perfect:\n",
    "                return #self.hdc,self.hde - unnecessary now that hdc and hde are within a class\n",
    "            print(\"regeneration\")\n",
    "            #do the dimension drop and regeneration\n",
    "            trainencoded=self.regen(hdb,amountDrop,traindata)\n",
    "        return \"error\",\"error\"\n",
    "    \n",
    "    def trainreploop(self,epochs,trainencoded,trainlabels):\n",
    "        # Do the train \n",
    "        for j in range(epochs):\n",
    "            # do one pass of training\n",
    "            train_acc = 100 * self.hdc.fit(trainencoded, trainlabels, self.param)\n",
    "            #Test accuracy used to be in here but I took it out because we dont usually know it\n",
    "            print(\"Train: %.2f \\t \\t Test: \"%(train_acc))\n",
    "            # If accuracy is 100, finish\n",
    "            if train_acc == 100:\n",
    "                return True\n",
    "        return False\n",
    "    def regen(self,hdb,amountDrop,traindata):\n",
    "        #make order of dimensions according to variaince; also store variances <-- unnecessary?\n",
    "        var, orders = self.hdc.evaluateBasis()\n",
    "        #drop dimensions with lowest variance\n",
    "        toDrop = orders[:amountDrop]\n",
    "        # print(\"Variances stats: max %.2f, min %.2f, mean %.2f\"%(max(var),min(var),np.mean(var)))\n",
    "        #update basis by randomizing dimension\n",
    "        hdb.updateBasis(toDrop)\n",
    "        # move the new basis into the encoder\n",
    "        self.hde.updateBasis(hdb.basis)\n",
    "        # normalize previous classes so retrain has enough effect\n",
    "        self.hdc.updateClasses()\n",
    "        # get new encoded training data\n",
    "        return self.hde.encodeData(traindata)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=NeuralHD(5,xtrain.shape[1],30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating times: 5\n",
      "regenloop: 0\n",
      "Train: 98.24 \t \t Test: \n",
      "Train: 98.54 \t \t Test: \n",
      "regeneration\n",
      "Updating basis......\n",
      "regenloop: 1\n",
      "Train: 98.41 \t \t Test: \n",
      "Train: 98.59 \t \t Test: \n",
      "regeneration\n",
      "Updating basis......\n",
      "regenloop: 2\n",
      "Train: 98.46 \t \t Test: \n",
      "Train: 98.60 \t \t Test: \n",
      "regeneration\n",
      "Updating basis......\n",
      "regenloop: 3\n",
      "Train: 98.48 \t \t Test: \n",
      "Train: 98.61 \t \t Test: \n",
      "regeneration\n",
      "Updating basis......\n",
      "regenloop: 4\n",
      "Train: 98.47 \t \t Test: \n",
      "Train: 98.65 \t \t Test: \n",
      "regeneration\n",
      "Updating basis......\n",
      "regenloop: 5\n",
      "Train: 98.46 \t \t Test: \n",
      "Train: 98.60 \t \t Test: \n"
     ]
    }
   ],
   "source": [
    "model.fit(xtrain,ytrain,2,5,.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0 accuracy: 0.9954973393368809\n",
      "points: 24430\n",
      "class 1 accuracy: 0.0\n",
      "points: 16\n",
      "class 2 accuracy: 0.9985575890254007\n",
      "points: 97753\n",
      "class 3 accuracy: 0.17346938775510204\n",
      "points: 294\n",
      "class 4 accuracy: 0.49358341559723595\n",
      "points: 1013\n"
     ]
    }
   ],
   "source": [
    "for i in range (0,model.param['nClasses']):\n",
    "    yhat= model(x_test[y_test==i])\n",
    "    acc = (yhat==i).mean()\n",
    "    print('class '+str(i)+' accuracy: ' +str(acc))\n",
    "    print('points: '+ str(len(yhat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate one random vector of desired length and generation type\n",
    "def generate_vector(vector_length, vector_type, param):\n",
    "    if vector_type == \"Gaussian\":\n",
    "        mu = param[\"mu\"]\n",
    "        sigma = param[\"sigma\"]\n",
    "        return np.random.normal(mu, sigma, vector_length)\n",
    "    else:\n",
    "        raise Exception(\"Vector type %s not recognized. Abort.\\n\" % vector_type)\n",
    "\n",
    "def vanilla(param):\n",
    "\n",
    "    #sys.stderr.write(\"Generating vanilla HD basis of shape... \")\n",
    "    basis = []\n",
    "    #for i in range(param[\"D\"]):\n",
    "    #for _ in tqdm_notebook(range(self.param[\"D\"]), desc='vectors'):\n",
    "    for _ in range(param[\"D\"]):\n",
    "        basis.append(generate_vector(param[\"nFeatures\"], param[\"vector\"], param))\n",
    "    basis = np.asarray(basis)\n",
    "\n",
    "# Load basis from a file\n",
    "\n",
    "\n",
    "\n",
    "        #sys.stderr.write(str(self.basis.shape)+\"\\n\")\n",
    "def updateBasis(basis, param, toChange = None):\n",
    "    print(\"Updating basis......\")# at the following indices: (None means changing everything)\")\n",
    "    #print(toChange)\n",
    "    for i in toChange:\n",
    "        basis[i] = generate_vector(param[\"nFeatures\"], param[\"vector\"], param)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'param' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-682323cd75da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'param' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
