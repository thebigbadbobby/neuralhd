{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import torch\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "class QHDModel(object):\n",
    "    def __init__(self, n_obs, n_actions, lr=0.05, lr_decay_rate=0.95, dimension=1000):\n",
    "        self.n_obs = n_obs\n",
    "        self.n_actions = n_actions\n",
    "        self.D = dimension\n",
    "        self.lr=lr\n",
    "        self.lr_decay_rate = lr_decay_rate\n",
    "\n",
    "        # Initialize HDModel\n",
    "        self.model = []\n",
    "        for a in range(self.n_actions):\n",
    "            self.model.append(np.zeros(self.D, dtype=complex))\n",
    "\n",
    "        # Initialize HDVec\n",
    "        self.s_hdvec = []\n",
    "        for n in range(self.n_obs):\n",
    "            self.s_hdvec.append(np.random.normal(0, 1, self.D))\n",
    "\n",
    "        # Initialize Bias\n",
    "        self.bias = np.random.uniform(0,2*np.pi, size=self.D)\n",
    "\n",
    "        # Initialize Delay Model\n",
    "        self.model = np.array(self.model)\n",
    "        self.delay_model = torch.from_numpy(copy.deepcopy(self.model))\n",
    "\n",
    "        # Convert to tensors\n",
    "        self.s_hdvec = torch.FloatTensor(self.s_hdvec)\n",
    "        self.model = torch.tensor(self.model, dtype=torch.cfloat)\n",
    "        self.delay_model = torch.tensor(self.delay_model, dtype=torch.cfloat)\n",
    "        self.bias = torch.FloatTensor(self.bias)\n",
    "\n",
    "    def update_delay_model(self):\n",
    "        self.delay_model = self.model.detach().clone()\n",
    "\n",
    "    # def keep_best_model(self, best_reward, best_step, current_reward, current_step):\n",
    "    #     if current_reward > best_reward:\n",
    "    #         best_reward = current_reward\n",
    "    #         best_step = current_step\n",
    "    #     return best_reward, best_step\n",
    "\n",
    "    def get_q_value(self, state):\n",
    "        # if torch.rand(1) <= epsilon:\n",
    "        #     return np.random.choice(self.n_actions)\n",
    "        # else:\n",
    "            # if type(state) == torch.Tensor:\n",
    "            #     state = state.to(self.GPU_device)\n",
    "            # else:\n",
    "            #     state = torch.tensor(state, dtype=torch.float64).to(self.GPU_device)\n",
    "\n",
    "        # Calculate Best Q Value\n",
    "        encoded = torch.exp(1j* (torch.matmul(state, self.s_hdvec) + self.bias))  #(1 x n_obs)*(n_obs x D)=(1 x D)\n",
    "        q_values = torch.real(torch.matmul(torch.conj(encoded), self.model.t()) / self.D)  # (1 x D)*(D x n_actions)=(1 x n_actions)\n",
    "        # best_action = int(torch.argmax(q_values))\n",
    "        return q_values\n",
    "\n",
    "    def update_model(self, buffer, batch_size, attacker=True, gamma=0.99):\n",
    "        minibatch = buffer.sample(batch_size)\n",
    "\n",
    "        s_attack, s_defend, a_attack, a_defend, r_attack, r_defend, is_done, ns_attack, ns_defend = minibatch\n",
    "\n",
    "        # Differentiate between attack and defense\n",
    "        if attacker:\n",
    "            rewards = torch.tensor(r_attack).float()\n",
    "            states = torch.tensor(s_attack).float()\n",
    "            next_states = torch.tensor(ns_attack).float()\n",
    "            actions = torch.tensor(a_attack).float()\n",
    "        else:\n",
    "            rewards = torch.tensor(r_defend).float()\n",
    "            states = torch.tensor(s_defend).float()\n",
    "            next_states = torch.tensor(ns_defend).float()\n",
    "            actions = torch.tensor(a_defend).float()\n",
    "        is_done = torch.tensor(is_done)\n",
    "\n",
    "        # Move to GPU if using GPU\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda:0\")\n",
    "            rewards = rewards.to(device)\n",
    "            states = states.to(device)\n",
    "            next_states = next_states.to(device)\n",
    "\n",
    "        # Encode Current and Next States\n",
    "        encoded = torch.exp(1j* (torch.matmul(states, self.s_hdvec) + self.bias))\n",
    "        encoded_ = torch.exp(1j* (torch.matmul(next_states, self.s_hdvec) + self.bias))\n",
    "\n",
    "        # Predict State\n",
    "        actions = [int(a) for a in actions]\n",
    "        y_pred = torch.real(torch.sum(torch.conj(encoded) * self.model[actions], dim=1) / self.D)\n",
    "\n",
    "        a_ = np.arange(self.n_actions)\n",
    "        q_values = torch.real(torch.matmul(torch.conj(encoded_), self.delay_model[a_].t()) / self.D)\n",
    "        max_q_values, _ = torch.max(q_values, dim=1)\n",
    "\n",
    "        # Use Bellman Equation to calculate actual loss\n",
    "        y_true = rewards + gamma * max_q_values\n",
    "        y_true = torch.where(is_done, rewards, y_true)  # if done, then we use actual reward, not Bellman Eqn Estimate\n",
    "\n",
    "        loss = torch.square(y_true - y_pred)\n",
    "\n",
    "        for i, action in enumerate(actions):\n",
    "            self.model[action] += self.lr * (y_true[i] - y_pred[i]) * encoded[i, :]\n",
    "\n",
    "        return torch.mean(loss)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
