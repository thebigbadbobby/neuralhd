{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SVM import SVM\n",
    "from MLP import MLP\n",
    "from OnlineHD_model import OnlineHD\n",
    "from OnlineHDv1 import OnlineHDv1\n",
    "from OnlineHDv2 import OnlineHDv2\n",
    "from NeuralHDv1 import NeuralHDv1\n",
    "from NeuralHDv2 import NeuralHDv2\n",
    "from testcode import NeuralHDSpecial\n",
    "from OnlineHD_model import OnlineHD\n",
    "from BIC import NeuralHDBIC\n",
    "from flexHD import NeuralHDDev\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"../../../Data/malware-classification/\"\n",
    "with open(path+'data11.npy', 'rb') as f:\n",
    "    transxtrain=np.load(f)\n",
    "    transx_test=np.load(f)\n",
    "    transytrain=np.load(f)\n",
    "    transy_test=np.load(f)\n",
    "xtr=torch.from_numpy(transxtrain).float()\n",
    "xte=torch.from_numpy(transx_test).float()\n",
    "ytr=torch.from_numpy(transytrain).long()\n",
    "yte=torch.from_numpy(transy_test).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General notes:\n",
    "#Try outputting arrays containing accuracy for each hyperparameter setting on your report.\n",
    "#That way, you will be able to easily paste the results into the excel spreadsheet.\n",
    "\n",
    "#Tine tuning parameters one by one using a for loop is prone to local minima.\n",
    "#The order in which the parameters tuned determines which local minimum is found.\n",
    "#Try running fine tuning multiple times starting with tuning a different parameter each time.\n",
    "#For example, last time you started with learning rate, so maybe start with another such as epochs\n",
    "# or dropout rate.\n",
    "#Try using initial static hyperparameters from my code, and also separately trying initial static\n",
    "#hyperparameters from the fine-tuning you already did once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLP\n",
    "#Compare my version to best version from your analysis (they look extremely similar in accuracy,\n",
    "#around 96.) Since parameters are quite different, this is evidence of local minima.\n",
    "#Hyperparameter fine tuning method is not perfect - possibility that neither of these is the best\n",
    "#Try rerunning the fine tuning starting with mine, and also with yours. \n",
    "#See if you can find hyperparameters that give better accuracy than 96.1\n",
    "#Report: Better hyperparameters and their resulting accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MLP(classes =9, features=257, dim = 1048)\n",
    "#training\n",
    "start=time.time()\n",
    "model.fit(transxtrain,transytrain,64,75,.001)\n",
    "end=time.time()\n",
    "training=end-start\n",
    "#inference\n",
    "start=time.time()\n",
    "yhat=model(transx_test)\n",
    "end=time.time()\n",
    "inference=end-start\n",
    "eval=[yhat[i]==transy_test[i] for i in range(len(transy_test))]\n",
    "acc=sum(eval)/len(transy_test)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperdimensional Computing Functions (OnlineHD, NeuralHD, NeuralHDSpecial exotic functions such\n",
    "# as BIC and SingleStop)\n",
    "#Note about batch size:\n",
    "#Treat batch size as a special parameter - if you run it at batch size close to 1, results will\n",
    "# be better, but it will take forever. \n",
    "# Fine tune all the other parameters with batch size 64, and only after all fine tuning is done \n",
    "# check how accuracy is if you lower batch size. This way, batch size doesn't slow down tuning\n",
    "# of the other parameters.\n",
    "\n",
    "# Make sure to try dropout=0 and regens=0 for NeuralHD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"Original\" Functions\n",
    "#OnlineHDv1 and NeuralHDv1\n",
    "#Try the same tuning strategy as MLP. In addition to the initial parameters below, consider the\n",
    "#optimal parameters you found for OnlineHD and NeuralHD with train1 function\n",
    "#Report: Better hyperparameters and their resulting accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OnlineHdv1\n",
    "model=OnlineHDv1(classes =9, features =257, dim = 2048, batch_size=64,lr=.0003)\n",
    "print(\"training\")\n",
    "start=time.time()\n",
    "model.fit(xtr,ytr,75)\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "print(\"inference\")\n",
    "start=time.time()\n",
    "yhat=model(xte)\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "eval=[yhat[i]==transy_test[i] for i in range(len(transy_test))]\n",
    "acc=sum(eval)/len(transy_test)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=NeuralHDv1(classes =9, features =257, dim = 2048, batch_size=64,lr=.0001)\n",
    "print(\"training\")\n",
    "start=time.time()\n",
    "model.fit(xtr,ytr,35,4,.1)\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "print(\"inference\")\n",
    "start=time.time()\n",
    "yhat=model(xte)\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "eval=[yhat[i]==transy_test[i] for i in range(len(transy_test))]\n",
    "acc=sum(eval)/len(transy_test)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"Improved\" Functions\n",
    "#OnlineHDv2 and NeuralHDv2\n",
    "#Try the tuning. In addition to the initial parameters below, consider the\n",
    "#optimal parameters you found for OnlineHD and NeuralHD with train3 function\n",
    "#Report: Better hyperparameters and their resulting accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=OnlineHDv2(classes =9, features =257, dim = 2048, batch_size=64,lr=.0001)\n",
    "print(\"training\")\n",
    "start=time.time()\n",
    "model.fit(xtr,ytr,75)\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "print(\"inference\")\n",
    "start=time.time()\n",
    "yhat=model(xte)\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "eval=[yhat[i]==transy_test[i] for i in range(len(transy_test))]\n",
    "acc=sum(eval)/len(transy_test)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=NeuralHDv2(classes =9, features =257, dim = 2048, batch_size=64,lr=.0001, multiencoder=False)\n",
    "print(\"training\")\n",
    "start=time.time()\n",
    "model.fit(xtr,ytr,35,5,.1)\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "print(\"inference\")\n",
    "start=time.time()\n",
    "yhat=model(xte)\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "eval=[yhat[i]==transy_test[i] for i in range(len(transy_test))]\n",
    "acc=sum(eval)/len(transy_test)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"Experimental\" Functions\n",
    "#NeuralHDSpecial BIC=True, \"SingleStop\"\n",
    "#Try the tuning. See if the same parameters that are good for NeuralHDv2 normal are also good\n",
    "#for BIC version, and figure out if optimal BIC version has better accuracy than optimal \n",
    "#Report: hyperparameters, accuracy, and is it better than NeuralHDv2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=NeuralHDSpecial(9,257,2048,batch_size=64,trainopt=2,bestinclass=True,lr=.0001)\n",
    "print(\"training\")\n",
    "start=time.time()\n",
    "model.fit(xtr,ytr,35,5,.1)\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "print(\"inference\")\n",
    "start=time.time()\n",
    "yhat=model(xte)\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "eval=[yhat[i]==transy_test[i] for i in range(len(transy_test))]\n",
    "acc=sum(eval)/len(transy_test)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A \"Single Stop\" is when you run model.fit normally, but when it stops, you run another model\n",
    "# of batch size 1. For some reason, this works only when you don't use have a regen in the first\n",
    "# fit.\n",
    "# Feel free to experiment with this strange effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "model=NeuralHDDev(9,257,1024,trainopt=2,bestinclass=True, multiencoder=True)\n",
    "model.fit(xtr,ytr,45,0,0,.0001,64,1)#Initial training\n",
    "model.fit(xtr,ytr,15,1,0,.0001,1,1)# stop training\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "print(\"inference\")\n",
    "start=time.time()\n",
    "yhat=model(xte)\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "eval=[yhat[i]==transy_test[i] for i in range(len(transy_test))]\n",
    "acc=sum(eval)/len(transy_test)\n",
    "print(acc)\n",
    "model.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "# check https://scikit-learn.org/stable/modules/svm.html\n",
    "#Accuracy is suspiciously low - look for ways to use different kernels such as rbf kernel\n",
    "#look for ways to optimize hyperparametes, such as GridSearchCV\n",
    "#Make sure to read: Section 1.4.6.1.\n",
    "#Report: Methods tried and their accuracy compared with SVM.py\n",
    "#Replace: The predict function in the SVM class with any function that has higher accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.620640993118286\n",
      "inference\n",
      "2.530949115753174\n"
     ]
    }
   ],
   "source": [
    "model=SVM()\n",
    "start=time.time()\n",
    "model.fit(transxtrain,transytrain)\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "print(\"inference\")\n",
    "start=time.time()\n",
    "yhat=model(transx_test)\n",
    "end=time.time()\n",
    "print(end-start)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
