{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import time\n",
    "import torch\n",
    "import sklearn.datasets\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.datasets import FashionMNIST as FMNIST\n",
    "from torchvision.datasets import EMNIST\n",
    "import torchvision.transforms as transforms\n",
    "import tensorflow as tf\n",
    "import random as r\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam, SGD, Adagrad, Adadelta, RMSprop\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Activation, BatchNormalization\n",
    "import sklearn\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import joblib\n",
    "from tqdm import tqdm_notebook\n",
    "import copy\n",
    "\n",
    "import Config\n",
    "import Dataloader as DL\n",
    "import HD_basis as HDB\n",
    "import HD_encoder as HDE\n",
    "import HD_classifier as HDC\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "42\n",
      "(494021,)\n",
      "(494021, 41)\n",
      "{'normal': 0, 'u2r': 1, 'dos': 2, 'r2l': 3, 'probe': 4}\n"
     ]
    }
   ],
   "source": [
    "path=\"../../Data/\"\n",
    "attacks_types = {\n",
    "    'normal': 'normal','back': 'dos','buffer_overflow': 'u2r','ftp_write': 'r2l','guess_passwd': 'r2l',\n",
    "'imap': 'r2l','ipsweep': 'probe','land': 'dos','loadmodule': 'u2r','multihop': 'r2l','neptune': 'dos',\n",
    "'nmap': 'probe','perl': 'u2r','phf': 'r2l','pod': 'dos','portsweep': 'probe','rootkit': 'u2r','satan': 'probe',\n",
    "'smurf': 'dos','spy': 'r2l','teardrop': 'dos','warezclient': 'r2l','warezmaster': 'r2l',\n",
    "}\n",
    "cols =\"\"\"duration,protocol_type,service,flag,src_bytes,dst_bytes,land,wrong_fragment,\n",
    "urgent,hot,num_failed_logins,logged_in,num_compromised,root_shell,su_attempted,num_root,\n",
    "num_file_creations,num_shells,num_access_files,num_outbound_cmds,is_host_login,is_guest_login,\n",
    "count,srv_count,serror_rate,srv_serror_rate,rerror_rate,srv_rerror_rate,same_srv_rate,\n",
    "diff_srv_rate,srv_diff_host_rate,dst_host_count,dst_host_srv_count,dst_host_same_srv_rate,\n",
    "dst_host_diff_srv_rate,dst_host_same_src_port_rate,dst_host_srv_diff_host_rate,\n",
    "dst_host_serror_rate,dst_host_srv_serror_rate,dst_host_rerror_rate,dst_host_srv_rerror_rate\"\"\"\n",
    "  \n",
    "columns =[]\n",
    "for c in cols.split(','):\n",
    "    if(c.strip()):\n",
    "       columns.append(c.strip())\n",
    "print(len(columns))\n",
    "columns.append('target')\n",
    "print(len(columns))\n",
    "\n",
    "attack_categories=[\"dos\",\"u2r\",\"r2l\",'probe','normal']\n",
    "df = pd.read_csv(path+\"kddcup.data_10_percent.gz\", names = columns)\n",
    "df['Attack Type'] = df.target.apply(lambda r:attacks_types[r[:-1]])\n",
    "del df['target']\n",
    "df.head()\n",
    "num_cols = df._get_numeric_data().columns\n",
    "  \n",
    "cate_cols = list(set(df.columns)-set(num_cols))\n",
    "cate_cols.remove('Attack Type')\n",
    "def getuniquevalues(columnname):\n",
    "    values={}\n",
    "    i=0\n",
    "    for entry in df[columnname]:\n",
    "        if entry not in values:\n",
    "            values[entry]=i\n",
    "            i+=1\n",
    "    return values\n",
    "for col in cate_cols:\n",
    "    df[col]=df[col].map(getuniquevalues(col))\n",
    "data=df.to_numpy()\n",
    "Y=df['Attack Type'].map(getuniquevalues('Attack Type'))\n",
    "Y=Y.to_numpy()\n",
    "X=data[:,:-1]\n",
    "print(Y.shape)\n",
    "print(X.shape)\n",
    "print(getuniquevalues('Attack Type'))\n",
    "def normalized(x,y):\n",
    "    x, x_test, y, y_test = sklearn.model_selection.train_test_split(x, y, shuffle=True)\n",
    "    scaler = sklearn.preprocessing.Normalizer().fit(x)\n",
    "    x = scaler.transform(x)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    # changes data to pytorch's tensors\n",
    "    x = torch.from_numpy(x).float()\n",
    "    y = torch.from_numpy(y).long()\n",
    "    x_test = torch.from_numpy(x_test).float()\n",
    "    y_test = torch.from_numpy(y_test).long()\n",
    "    return x.numpy(), x_test.numpy(), y.numpy(), y_test.numpy(), scaler\n",
    "xtrain, x_test, ytrain, y_test,scaler= normalized(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(hdc, traindata, trainlabels, testdata, testlabels, param = Config.config, epochs = None):\n",
    "#     train_acc = []\n",
    "#     test_acc = []\n",
    "#     if epochs is not None:\n",
    "#         param[\"epochs\"] = epochs\n",
    "#     #for i in tqdm_notebook(range(param[\"epochs\"]), desc='epochs'):\n",
    "#     for i in range(param[\"epochs\"]):\n",
    "#         train_acc.append(hdc.fit(traindata, trainlabels, param))\n",
    "#         test_acc.append(hdc.test(testdata, testlabels))\n",
    "#         # if len(train_acc) % 20 == 0:\n",
    "#         #     print(\"Train: %f \\t \\t Test: %f\"%(train_acc[-1], test_acc[-1]))\n",
    "#         # if train_acc[-1] == 1:\n",
    "#         #     print(\"Training converged!\") \n",
    "#         #     print(\"Train: %f \\t \\t Test: %f\"%(train_acc[-1], test_acc[-1]))\n",
    "#         #     break\n",
    "\n",
    "#     return np.asarray(train_acc), np.asarray(test_acc), i\n",
    "def trainreploop(hdc,trainreps,trainencoded,trainlabels,param):\n",
    "    # Do the train \n",
    "    for j in range(trainreps):\n",
    "        train_acc = 100 * hdc.fit(trainencoded, trainlabels, param)\n",
    "        # test_acc = 100 * hdc.test(testencoded, testlabels)\n",
    "        # train_accs.append(train_acc)\n",
    "        # test_accs.append(test_acc)\n",
    "        print(\"Train: %.2f \\t \\t Test: \"%(train_acc))\n",
    "        if train_acc == 100:\n",
    "            break\n",
    "def regen(hdb,hdc,hde,amountDrop,traindata):\n",
    "    var, orders = hdc.evaluateBasis()\n",
    "    toDrop = orders[:amountDrop]\n",
    "    # toMask = orders[-amountDrop:]\n",
    "    # toDropVar = [var[i] for i in toDrop]\n",
    "    print(\"Variances stats: max %.2f, min %.2f, mean %.2f\"%(max(var),min(var),np.mean(var)))\n",
    "    #print(\"Dropping first %f percent of ineffective basis, with stats: max %f, min %f, mean %f\"\\\n",
    "    #      %(percentDrop, max(toDropVar),min(toDropVar),np.mean(toDropVar)))\n",
    "    hdb.updateBasis(toDrop)\n",
    "    hde.updateBasis(hdb.basis)\n",
    "    hdc.updateClasses()\n",
    "    return hde.encodeData(traindata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_ed(traindata, trainlabels, testdata, testlabels,\n",
    "                   D, # initial baseline\n",
    "                   regenloops,  # list of effective dimensions to reach \n",
    "                   percentDrop, # drop/regen rate \n",
    "                   trainreps, # # iterations per regen  \n",
    "                   param):\n",
    "\n",
    "    param[\"D\"] = D\n",
    "    \n",
    "    # Initialize basis & classifier\n",
    "    hdb = HDB.HD_basis(HDB.Generator.Vanilla, param)\n",
    "    basis = hdb.getBasis()\n",
    "    param = hdb.getParam()\n",
    "    hde = HDE.HD_encoder(basis)\n",
    "    trainencoded = hde.encodeData(traindata)\n",
    "    # testencoded = hde.encodeData(testdata)\n",
    "    # Initialize classifier\n",
    "    # train_accs = []\n",
    "    # test_accs = []\n",
    "    hdc = HDC.HD_classifier(param[\"D\"], param[\"nClasses\"], 0)\n",
    "\n",
    "    # Prepare setting for train\n",
    "    amountDrop = int(percentDrop * hdc.D)\n",
    "    print(\"Updating times:\", regenloops)\n",
    "\n",
    "    # early_stopping_steps = 1000 # earlystopping is \"turned off\"\n",
    "\n",
    "    # #es_count = 0\n",
    "    # max_test = 0\n",
    "    # best = None\n",
    "    # best_idx = 0\n",
    "    \n",
    "    # # Checkpoints\n",
    "    # checkpoints = []\n",
    "\n",
    "    for i in range(regenloops+1): # For each eDs to reach, will checkpoints\n",
    "        print(\"regenloop: \" + str(i))\n",
    "        # Do the train \n",
    "        trainreploop(hdc,trainreps,trainencoded,trainlabels,param)\n",
    "        # for j in range(iter_per_update):\n",
    "        #     train_acc = 100 * hdc.fit(trainencoded, trainlabels, param)\n",
    "        #     # test_acc = 100 * hdc.test(testencoded, testlabels)\n",
    "        #     # train_accs.append(train_acc)\n",
    "        #     # test_accs.append(test_acc)\n",
    "        #     print(\"Train: %.2f \\t \\t Test: \"%(train_acc))\n",
    "        #     if train_acc == 100:\n",
    "        #         break\n",
    "         \n",
    "        # if train_acc == 100:\n",
    "        #     print(\"Train converged! taking snippit in checkpoints\")\n",
    "        #     hdb_ck = copy.deepcopy(hdb)\n",
    "        #     hdc_ck = copy.deepcopy(hdc)\n",
    "        #     _, post_test_accs, _ = train(hdc_ck, trainencoded, trainlabels, testencoded, testlabels, param, epochs = 50)\n",
    "            # checkpoints.append((i+1, (D + (i)*amountDrop), \n",
    "            #                     hdb_ck, hdc_ck, \n",
    "            #                     max(test_accs[-iter_per_update:]), max(post_test_accs)))\n",
    "\n",
    "        # if test_accs[-1] >= max_test:\n",
    "        #     es_count = 0\n",
    "        #     best = copy.deepcopy(hdc)\n",
    "        #     best_idx = len(test_accs)\n",
    "        # else:\n",
    "        #     es_count += 1\n",
    "        # if es_count > early_stopping_steps:\n",
    "        #     print(\"Early stopping initiated, best stores the best hdc currently\")\n",
    "        #     break\n",
    "        \n",
    "        # if i in regenTimes:\n",
    "        #     print(\"Checkpoint made!\")\n",
    "        #     hdb_ck = copy.deepcopy(hdb)\n",
    "        #     hdc_ck = copy.deepcopy(hdc)\n",
    "        #     _, post_test_accs, _ = train(hdc_ck, trainencoded, trainlabels, testencoded, testlabels, param, epochs = 50)\n",
    "        #     checkpoints.append((D, (D + (i)*amountDrop), \n",
    "        #                         None, None, #hdb_ck, hdc_ck, \n",
    "        #                         max(test_accs[-iter_per_update:]), max(post_test_accs)))\n",
    "        if i==regenloops:\n",
    "            return hdc,hde\n",
    "        print(\"regeneration\")\n",
    "        # Do the regeneration\n",
    "        # var, orders = hdc.evaluateBasis()\n",
    "        # toDrop = orders[:amountDrop]\n",
    "        # toMask = orders[-amountDrop:]\n",
    "        # toDropVar = [var[i] for i in toDrop]\n",
    "        # print(\"Variances stats: max %.2f, min %.2f, mean %.2f\"%(max(var),min(var),np.mean(var)))\n",
    "        # #print(\"Dropping first %f percent of ineffective basis, with stats: max %f, min %f, mean %f\"\\\n",
    "        # #      %(percentDrop, max(toDropVar),min(toDropVar),np.mean(toDropVar)))\n",
    "        # hdb.updateBasis(toDrop)\n",
    "        # hde.updateBasis(hdb.basis)\n",
    "        # trainencoded = hde.encodeData(traindata)\n",
    "        # testencoded = hde.encodeData(testdata)\n",
    "        # hdc.updateClasses()\n",
    "        trainencoded=regen(hdb,hdc,hde,amountDrop,traindata)\n",
    "    return \"error\",\"error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current config: 100 0.1 2\n",
      "Updating times: 5\n",
      "regenloop: 0\n",
      "Train: 98.34 \t \t Test: \n",
      "Train: 98.60 \t \t Test: \n",
      "regeneration\n",
      "Variances stats: max 0.05, min 0.00, mean 0.01\n",
      "Updating basis......\n",
      "regenloop: 1\n",
      "Train: 98.54 \t \t Test: \n",
      "Train: 98.70 \t \t Test: \n",
      "regeneration\n",
      "Variances stats: max 0.03, min 0.00, mean 0.01\n",
      "Updating basis......\n",
      "regenloop: 2\n",
      "Train: 98.58 \t \t Test: \n",
      "Train: 98.70 \t \t Test: \n",
      "regeneration\n",
      "Variances stats: max 0.03, min 0.00, mean 0.01\n",
      "Updating basis......\n",
      "regenloop: 3\n",
      "Train: 98.59 \t \t Test: \n",
      "Train: 98.74 \t \t Test: \n",
      "regeneration\n",
      "Variances stats: max 0.04, min 0.00, mean 0.01\n",
      "Updating basis......\n",
      "regenloop: 4\n",
      "Train: 98.61 \t \t Test: \n",
      "Train: 98.73 \t \t Test: \n",
      "regeneration\n",
      "Variances stats: max 0.04, min 0.00, mean 0.01\n",
      "Updating basis......\n",
      "regenloop: 5\n",
      "Train: 98.59 \t \t Test: \n",
      "Train: 98.75 \t \t Test: \n"
     ]
    }
   ],
   "source": [
    "D=100\n",
    "percentDrop=0.1\n",
    "regenloops=5\n",
    "trainreps = 2\n",
    "param = Config.config\n",
    "param[\"nFeatures\"] = xtrain.shape[1]\n",
    "param[\"nClasses\"] = len(df['Attack Type'].unique())\n",
    "\n",
    "#Ds = [200, 500, 750, 1000, 1500, 2000, 2500, 3000, 4000]\n",
    "#percentDrops = [0.05]\n",
    "#iter_per_updates = [1, 2, 3, 4]\n",
    "\n",
    "\n",
    "hdc=None\n",
    "# for iter_per_update in iter_per_updates:\n",
    "print(\"Current config:\", D, percentDrop, trainreps)\n",
    "hdc,hde = train_neural_ed(xtrain, ytrain, x_test, y_test,\n",
    "        D, # initial baseline\n",
    "        regenloops,\n",
    "        percentDrop, # drop/regen rate \n",
    "        trainreps, # # iterations per regen  \n",
    "        param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(hdc,hde,x):\n",
    "    trainencoded=hde.encodeData(x)\n",
    "    return hdc.predict(trainencoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-2866e39e5d41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0myhat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhde\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'bool' object has no attribute 'mean'"
     ]
    }
   ],
   "source": [
    "for i in range (0,len(np.unique(Y))):\n",
    "    yhat=predict(hdc,hde,x_test[y_test==i])\n",
    "    acc = (yhat==i).mean()\n",
    "    print(yhat==i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9780424264979531\n",
      "0.0\n",
      "0.9905367922989068\n",
      "0.8692307692307693\n",
      "0.9139676113360324\n"
     ]
    }
   ],
   "source": [
    "for i in range (0,len(np.unique(Y))):\n",
    "    yhat=np.array(predict(hdc,hde,x_test[y_test==i]))\n",
    "    acc = (yhat==i).mean()\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_location': './dataset/',\n",
       " 'directory': 'smart_home',\n",
       " 'dataset': 'smart_home_split',\n",
       " 'D': 100,\n",
       " 'vector': 'Gaussian',\n",
       " 'mu': 0,\n",
       " 'sigma': 1,\n",
       " 'binarize': 0,\n",
       " 'lr': 0.037,\n",
       " 'sparse': 0,\n",
       " 's': 0.1,\n",
       " 'binaryModel': 0,\n",
       " 'checkpoints': False,\n",
       " 'width': None,\n",
       " 'height': None,\n",
       " 'nLayers': 5,\n",
       " 'uniform_dim': 1,\n",
       " 'uniform_ker': 1,\n",
       " 'dArr': None,\n",
       " 'k': 3,\n",
       " 'kArr': None,\n",
       " 'one_shot': 0,\n",
       " 'data_percentages': [1.0, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5],\n",
       " 'train_percent': 1,\n",
       " 'dropout': 0,\n",
       " 'drop_percentages': [0, 0.1, 0.2, 0.5],\n",
       " 'dropout_rate': 0,\n",
       " 'update_type': <Update_T.FULL: 1>,\n",
       " 'masked': False,\n",
       " 'iter_per_trial': 3,\n",
       " 'iter_per_encoding': 5,\n",
       " 'epochs': 250,\n",
       " 'nFeatures': 41,\n",
       " 'nClasses': 5,\n",
       " 'id': '5420',\n",
       " 'gen_type': <Generator.Vanilla: 1>}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24616, 41)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a57d33d28885>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Attack Type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0myhat\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mhdc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# yhat=np.array([row.numpy().argmax() for row in yhat])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Files/Research/Cybersecurity/HD_classifier.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mD\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "    # yhat=np.array([row.numpy().argmax() for row in yhat])\n",
    "    # acc = (yhat==i).mean()\n",
    "    # print('class '+str(i)+' accuracy: ' +str(acc))\n",
    "    # print('points: '+ str(len(yhat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralHD:\n",
    "    def __init__(self, classes : int, features : int, dim : int = 400):\n",
    "        self.param=Config.config\n",
    "        self.param['nClasses'] = classes\n",
    "        self.param['nFeatures']= features\n",
    "        self.param['D']=dim\n",
    "        self.hde=None\n",
    "        self.hdc=None\n",
    "    def __call__(self, x : torch.Tensor):\n",
    "        assert hde!=None and hdc!=None\n",
    "        return self.predict(x)\n",
    "    def predict(self,x):\n",
    "        trainencoded=self.hde.encodeData(x)\n",
    "        return np.array(self.hdc.predict(trainencoded))\n",
    "    def fit(self,traindata, trainlabels,\n",
    "                   trainreps,\n",
    "                   regenloops,  # list of effective dimensions to reach \n",
    "                   percentDrop # drop/regen rate \n",
    "                    ):\n",
    "        \n",
    "        # Initialize basis & classifier\n",
    "        hdb = HDB.HD_basis(HDB.Generator.Vanilla, self.param)\n",
    "        basis = hdb.getBasis()\n",
    "        self.param = hdb.getParam()\n",
    "        self.hde = HDE.HD_encoder(basis)\n",
    "        trainencoded = self.hde.encodeData(traindata)\n",
    "        self.hdc = HDC.HD_classifier(self.param[\"D\"], self.param[\"nClasses\"], 0)\n",
    "\n",
    "        # Prepare setting for train\n",
    "        amountDrop = int(percentDrop * self.hdc.D)#self.param.D?\n",
    "        print(\"Updating times:\", regenloops)\n",
    "\n",
    "        for i in range(regenloops+1): # For each eDs to reach, will checkpoints\n",
    "            print(\"regenloop: \" + str(i))\n",
    "            # Do the train \n",
    "            self.trainreploop(trainreps,trainencoded,trainlabels)\n",
    "            if i==regenloops:\n",
    "                return self.hdc,self.hde\n",
    "            print(\"regeneration\")\n",
    "            trainencoded=self.regen(hdb,amountDrop,traindata)\n",
    "        return \"error\",\"error\"\n",
    "    \n",
    "    def trainreploop(self,trainreps,trainencoded,trainlabels):\n",
    "        # Do the train \n",
    "        for j in range(trainreps):\n",
    "            train_acc = 100 * self.hdc.fit(trainencoded, trainlabels, self.param)\n",
    "            # test_acc = 100 * hdc.test(testencoded, testlabels)\n",
    "            # train_accs.append(train_acc)\n",
    "            # test_accs.append(test_acc)\n",
    "            print(\"Train: %.2f \\t \\t Test: \"%(train_acc))\n",
    "            if train_acc == 100:\n",
    "                break\n",
    "    def regen(self,hdb,amountDrop,traindata):\n",
    "        var, orders = self.hdc.evaluateBasis()\n",
    "        toDrop = orders[:amountDrop]\n",
    "        # toMask = orders[-amountDrop:]\n",
    "        # toDropVar = [var[i] for i in toDrop]\n",
    "        print(\"Variances stats: max %.2f, min %.2f, mean %.2f\"%(max(var),min(var),np.mean(var)))\n",
    "        #print(\"Dropping first %f percent of ineffective basis, with stats: max %f, min %f, mean %f\"\\\n",
    "        #      %(percentDrop, max(toDropVar),min(toDropVar),np.mean(toDropVar)))\n",
    "        hdb.updateBasis(toDrop)\n",
    "        self.hde.updateBasis(hdb.basis)\n",
    "        self.hdc.updateClasses()\n",
    "        return self.hde.encodeData(traindata)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdc.counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=NeuralHD(5,xtrain.shape[1],300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating times: 5\n",
      "regenloop: 0\n",
      "Train: 98.34 \t \t Test: \n",
      "Train: 98.66 \t \t Test: \n",
      "regeneration\n",
      "Variances stats: max 0.03, min 0.00, mean 0.00\n",
      "Updating basis......\n",
      "regenloop: 1\n",
      "Train: 98.56 \t \t Test: \n",
      "Train: 98.71 \t \t Test: \n",
      "regeneration\n",
      "Variances stats: max 0.02, min 0.00, mean 0.00\n",
      "Updating basis......\n",
      "regenloop: 2\n",
      "Train: 98.54 \t \t Test: \n",
      "Train: 98.73 \t \t Test: \n",
      "regeneration\n",
      "Variances stats: max 0.02, min 0.00, mean 0.00\n",
      "Updating basis......\n",
      "regenloop: 3\n",
      "Train: 98.59 \t \t Test: \n",
      "Train: 98.75 \t \t Test: \n",
      "regeneration\n",
      "Variances stats: max 0.02, min 0.00, mean 0.00\n",
      "Updating basis......\n",
      "regenloop: 4\n",
      "Train: 98.58 \t \t Test: \n",
      "Train: 98.71 \t \t Test: \n",
      "regeneration\n",
      "Variances stats: max 0.02, min 0.00, mean 0.00\n",
      "Updating basis......\n",
      "regenloop: 5\n",
      "Train: 98.58 \t \t Test: \n",
      "Train: 98.75 \t \t Test: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<HD_classifier.HD_classifier at 0x108f4bfd0>,\n",
       " <HD_encoder.HD_encoder at 0x108d12e50>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain,ytrain,2,5,.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0 accuracy: 0.9983045941363768\n",
      "points: 24183\n",
      "class 1 accuracy: 0.0\n",
      "points: 11\n",
      "class 2 accuracy: 0.9923009463207701\n",
      "points: 98064\n",
      "class 3 accuracy: 0.15384615384615385\n",
      "points: 260\n",
      "class 4 accuracy: 0.8026315789473685\n",
      "points: 988\n"
     ]
    }
   ],
   "source": [
    "for i in range (0,len(df['Attack Type'].unique())):\n",
    "    yhat= model(x_test[y_test==i])\n",
    "    acc = (yhat==i).mean()\n",
    "    print('class '+str(i)+' accuracy: ' +str(acc))\n",
    "    print('points: '+ str(len(yhat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.hdc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
